{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887d2bd5",
   "metadata": {},
   "source": [
    "# Telco Customer Churn Data Science Showcase\n",
    "Strategic Finance Data Scientist Demo (OpenAI-style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078e3917",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Business Problem & Objectives\n",
    "\n",
    "**Objective:**  \n",
    "- Predict customer churn and estimate financial impact (revenue at risk).  \n",
    "- Demonstrate classical ML + LLM-powered insights.  \n",
    "- Show how data science supports strategic finance decisions.  \n",
    "\n",
    "**Success Metrics:**  \n",
    "- Model performance: ROC-AUC > 0.80.  \n",
    "- Business impact: estimate potential revenue saved by targeting top 20% at-risk customers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5a694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3066f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Dataset (Telco Customer Churn)\n",
    "df = pd.read_csv('data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42ec5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cleaning\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates()\n",
    "print(\"Shape after cleaning:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EDA Examples\n",
    "sns.countplot(x='Churn', data=df)\n",
    "plt.title('Churn Distribution')\n",
    "plt.show()\n",
    "\n",
    "sns.histplot(data=df, x='tenure', hue='Churn', multiple='stack', bins=30)\n",
    "plt.title('Tenure Distribution by Churn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ce1a8",
   "metadata": {},
   "source": [
    "## Feature Engineering Explained\n",
    "\n",
    "Feature engineering transforms raw data into meaningful inputs for machine learning models. In this project:\n",
    "- **Tenure Bucketing:** We group customers by how long they've been with the company (0-12, 13-24, 25-48, 49-72 months) to capture retention patterns.\n",
    "- **Categorical Encoding:** All categorical features (except customerID) are converted to numeric using one-hot encoding, allowing models to interpret them.\n",
    "- **Scaling Numeric Features:** Tenure, MonthlyCharges, and TotalCharges are standardized to have zero mean and unit variance, improving model convergence and comparability.\n",
    "- **Target and Features:** We drop identifiers and set up X (features) and y (target: churn).\n",
    "- **Train/Test Split:** Data is split into training and test sets for unbiased model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a9c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Feature Engineering\n",
    "\n",
    "# 1. Bucket tenure into groups to capture retention patterns\n",
    "df['tenure_bucket'] = pd.cut(df['tenure'], bins=[0,12,24,48,72], labels=['0-12','13-24','25-48','49-72'])\n",
    "\n",
    "# 2. Identify categorical columns (excluding customerID)\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.drop(['customerID'])\n",
    "\n",
    "# 3. One-hot encode categorical features\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "# 4. One-hot encode tenure buckets\n",
    "df = pd.get_dummies(df, columns=['tenure_bucket'], drop_first=True)\n",
    "\n",
    "# 5. Standardize numeric features for model stability\n",
    "scaler = StandardScaler()\n",
    "num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "# 6. Prepare features (X) and target (y)\n",
    "X = df.drop(['customerID', 'Churn_Yes'], axis=1)\n",
    "y = df['Churn_Yes']\n",
    "\n",
    "# 7. Split data into train and test sets (manual method for clarity)\n",
    "\n",
    "# Shuffle the data to randomize\n",
    "X_shuffled, y_shuffled = shuffle(X, y, random_state=42)\n",
    "\n",
    "# Calculate split index for 80% train, 20% test\n",
    "split_idx = int(0.8 * len(X_shuffled))\n",
    "\n",
    "# Training set\n",
    "X_train = X_shuffled[:split_idx]\n",
    "y_train = y_shuffled[:split_idx]\n",
    "\n",
    "# Test set\n",
    "X_test = X_shuffled[split_idx:]\n",
    "y_test = y_shuffled[split_idx:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d7e75",
   "metadata": {},
   "source": [
    "## Predictive Modeling Explained\n",
    "\n",
    "In this section, we train and evaluate three machine learning models to predict customer churn:\n",
    "\n",
    "\n",
    "- **Logistic Regression:** A simple, interpretable model that estimates the probability of churn based on input features.\n",
    "- **Random Forest:** An ensemble of decision trees that captures complex patterns and interactions in the data.\n",
    "- **XGBoost:** A powerful gradient boosting algorithm known for high accuracy and speed in tabular data tasks.\n",
    "\n",
    "\n",
    "For each model, we fit it to the training data, make predictions on the test set, and report key metrics:\n",
    "- **AUC (Area Under the ROC Curve):** Measures the model's ability to distinguish between churned and retained customers.\n",
    "- **Classification Report:** Shows precision, recall, and F1-score for each class.\n",
    "\n",
    "\n",
    "This approach allows us to compare model performance and select the best method for business decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c4bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predictive Modeling\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "models = {'LR': lr, 'RF': rf, 'XGB': xgb}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(f\"{name} AUC: {auc:.3f}\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab94c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, RocCurveDisplay\n",
    "importances = {}\n",
    "\n",
    "# ROC (Receiver Operating Characteristic) Curve for all models\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_proba = model.predict_proba(X_test)[:,1]\n",
    "    else:\n",
    "        y_proba = model.decision_function(X_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "    plt.plot(fpr, tpr, label=f'{name} (AUC: {roc_auc_score(y_test, y_proba):.2f})')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Comparison')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for all models\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix: {name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "# Feature Importance for tree models\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances[name] = model.feature_importances_\n",
    "        feat_imp = pd.Series(model.feature_importances_, index=X_test.columns).sort_values(ascending=False)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        feat_imp.head(10).plot(kind='barh')\n",
    "        plt.title(f'Top 10 Feature Importances: {name}')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ad7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainability (Random Forest Example)\n",
    "explainer = shap.TreeExplainer(rf)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# For binary classification, shap_values is a list of two arrays.\n",
    "# Use shap_values[1] only if its shape matches X_test, otherwise use shap_values.\n",
    "if isinstance(shap_values, list) and len(shap_values) == 2 and shap_values[1].shape == X_test.shape:\n",
    "    shap.summary_plot(shap_values[1], X_test)\n",
    "else:\n",
    "    shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7b037",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Scenario Simulation & Causal Inference (Stub)\n",
    "- Simulate impact if all customers move from month-to-month → 1-year contracts.  \n",
    "- Estimate uplift of offering discounts to top 20% at-risk customers.  \n",
    "- Use causal inference libraries (DoWhy, EconML) to measure treatment effects.  \n",
    "\n",
    "## 9. LLM-Powered Insights (Stub)\n",
    "- Pass churn results to OpenAI GPT to generate executive memos, insights, and scenarios.  \n",
    "- Example: \"If discount applied, projected retention improves by X%.\"  \n",
    "\n",
    "## 10. Dashboarding & Storytelling (Stub)\n",
    "- Streamlit/Plotly Dash app for churn risk + financial impact.  \n",
    "- Executive memo generation pipeline.  \n",
    "\n",
    "## 11. Next Steps: Automation & Reproducibility\n",
    "- Package code as Python modules.  \n",
    "- Automate data refresh & retraining.  \n",
    "- Integrate OpenAI API for reporting.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
